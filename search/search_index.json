{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my blog \u00b6 I'm a software engineer at Mainstream Renewable Power While attempting to modernise legacy Django codebase I've written down a few learnings in today-I-learned ( til ) format that perhaps others might find useful... Inspired by simonw/til","title":"Welcome to my blog"},{"location":"#welcome-to-my-blog","text":"I'm a software engineer at Mainstream Renewable Power While attempting to modernise legacy Django codebase I've written down a few learnings in today-I-learned ( til ) format that perhaps others might find useful... Inspired by simonw/til","title":"Welcome to my blog"},{"location":"_til/add-conda-to-the-powershell-profile/","text":"Add conda to the powershell profile \u00b6 I wanted to automatically activate conda in powershell on startup. In zsh I can edit the ~/.zshrc , in powershell the equivalent is the unwieldy C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 or $PROFILE . conda supports editing this profile by running conda init powershell in the Anaconda powershell prompt that comes installed with miniconda or Anaconda . By default powershell does not allow the user to run unsigned scripts! To override this I had to change the execution policy to RemoteSigned for the CurrentUser as I don't have admin rights beyond this. Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser From the microsoft docs RemoteSigned means: Requires a digital signature from a trusted publisher on scripts and configuration files that are downloaded from the internet which includes email and instant messaging programs. Doesn't require digital signatures on scripts that are written on the local computer and not downloaded from the internet. Runs scripts that are downloaded from the internet and not signed, if the scripts are unblocked, such as by using the Unblock-File cmdlet. Risks running unsigned scripts from sources other than the internet and signed scripts that could be malicious. Now powershell boots with conda initialised and visible in the prompt! This means that the powershell shell in vscode now has access to conda . It also means that I can now style my prompt using oh-my-posh (see Styling powershell with oh-my-posh ) and make it permanent. :exclamation: See profile here powershell \u00b6","title":"Add conda to the powershell profile"},{"location":"_til/add-conda-to-the-powershell-profile/#add-conda-to-the-powershell-profile","text":"I wanted to automatically activate conda in powershell on startup. In zsh I can edit the ~/.zshrc , in powershell the equivalent is the unwieldy C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\WindowsPowerShell\\Microsoft.PowerShell_profile.ps1 or $PROFILE . conda supports editing this profile by running conda init powershell in the Anaconda powershell prompt that comes installed with miniconda or Anaconda . By default powershell does not allow the user to run unsigned scripts! To override this I had to change the execution policy to RemoteSigned for the CurrentUser as I don't have admin rights beyond this. Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser From the microsoft docs RemoteSigned means: Requires a digital signature from a trusted publisher on scripts and configuration files that are downloaded from the internet which includes email and instant messaging programs. Doesn't require digital signatures on scripts that are written on the local computer and not downloaded from the internet. Runs scripts that are downloaded from the internet and not signed, if the scripts are unblocked, such as by using the Unblock-File cmdlet. Risks running unsigned scripts from sources other than the internet and signed scripts that could be malicious. Now powershell boots with conda initialised and visible in the prompt! This means that the powershell shell in vscode now has access to conda . It also means that I can now style my prompt using oh-my-posh (see Styling powershell with oh-my-posh ) and make it permanent. :exclamation: See profile here","title":"Add conda to the powershell profile"},{"location":"_til/add-conda-to-the-powershell-profile/#powershell","text":"","title":"powershell"},{"location":"_til/backup-files-from-wsl-to-onedrive/","text":"Backup files from wsl to onedrive \u00b6 I want to backup source code saved in a Windows Subsystems for Linux (WSL) subfolder to OneDrive daily to add an extra degree of redundancy on top of GitHub . I have a local OneDrive folder that syncs automatically and Task Scheduler can run bat files at a given time. I created a simple file backup-stationmanager.bat ... xcopy ^ \"\\\\wsl$\\Ubuntu\\home\\rdmolony\\Code\\StationManager\\.git\" ^ \"C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\Backups\\StationManager\\.git\" ^ /y /s ... that uses ^ instead of \\ for multiline, \\y to accept all and \\s to copy subfolders windows \u00b6","title":"Backup files from wsl to onedrive"},{"location":"_til/backup-files-from-wsl-to-onedrive/#backup-files-from-wsl-to-onedrive","text":"I want to backup source code saved in a Windows Subsystems for Linux (WSL) subfolder to OneDrive daily to add an extra degree of redundancy on top of GitHub . I have a local OneDrive folder that syncs automatically and Task Scheduler can run bat files at a given time. I created a simple file backup-stationmanager.bat ... xcopy ^ \"\\\\wsl$\\Ubuntu\\home\\rdmolony\\Code\\StationManager\\.git\" ^ \"C:\\Users\\Rowan.Molony\\OneDrive - Mainstream Renewable Power\\Documents\\Backups\\StationManager\\.git\" ^ /y /s ... that uses ^ instead of \\ for multiline, \\y to accept all and \\s to copy subfolders","title":"Backup files from wsl to onedrive"},{"location":"_til/backup-files-from-wsl-to-onedrive/#windows","text":"","title":"windows"},{"location":"_til/block-duplicate-records-being-saved-in-django/","text":"Block duplicate records being saved in django \u00b6 I want to save unique flags indicating faulty time-series data to a database. I want Django to raise an error if an attempt is made to save a record that already exists. I could write some logic to catch this prior to writing the data each time but I'd rather have a validator at the model level. I thought that setting the unique=True constraint on each DateTimeField would prevent this. It didn't. It seems this constraint is intended to prevent duplicates across an entire field rather than across fields - so say \"Ireland\" can't be written twice to a field called \"countries\". I can instead validate by checking new values against existing values in the database prior to saving: class Flag(models.Model): id = models.AutoField(primary_key=True, db_column='id', blank=False, null=False) flag = models.CharField(db_column='Notes', null=False, blank=False, max_length=255) start_timestamp = models.DateTimeField(db_column='Start_Timestamp', blank=False, null=False) stop_timestamp = models.DateTimeField(db_column='Stop_Timestamp', blank=False, null=False) def is_a_new_record(self) -> bool: record_exists = Flag.objects.filter( flag=self.flag, start_timestamp=self.start_timestamp, stop_timestamp=self.stop_timestamp ).exists() return not record_exists def save(self, *args, **kwargs): if self.is_a_new_record(): return super().save(*args, **kwargs) else: print(f\"{self} is not a new record!\") django \u00b6","title":"Block duplicate records being saved in django"},{"location":"_til/block-duplicate-records-being-saved-in-django/#block-duplicate-records-being-saved-in-django","text":"I want to save unique flags indicating faulty time-series data to a database. I want Django to raise an error if an attempt is made to save a record that already exists. I could write some logic to catch this prior to writing the data each time but I'd rather have a validator at the model level. I thought that setting the unique=True constraint on each DateTimeField would prevent this. It didn't. It seems this constraint is intended to prevent duplicates across an entire field rather than across fields - so say \"Ireland\" can't be written twice to a field called \"countries\". I can instead validate by checking new values against existing values in the database prior to saving: class Flag(models.Model): id = models.AutoField(primary_key=True, db_column='id', blank=False, null=False) flag = models.CharField(db_column='Notes', null=False, blank=False, max_length=255) start_timestamp = models.DateTimeField(db_column='Start_Timestamp', blank=False, null=False) stop_timestamp = models.DateTimeField(db_column='Stop_Timestamp', blank=False, null=False) def is_a_new_record(self) -> bool: record_exists = Flag.objects.filter( flag=self.flag, start_timestamp=self.start_timestamp, stop_timestamp=self.stop_timestamp ).exists() return not record_exists def save(self, *args, **kwargs): if self.is_a_new_record(): return super().save(*args, **kwargs) else: print(f\"{self} is not a new record!\")","title":"Block duplicate records being saved in django"},{"location":"_til/block-duplicate-records-being-saved-in-django/#django","text":"","title":"django"},{"location":"_til/cache-package-manager-installs-with-buildkit/","text":"Cache package manager installs with buildkit \u00b6 Downloading project dependencies for every Docker rebuild is slow. buildkit enables caching downloads locally between builds thus skipping this download. Lifesaver. apt : # g++, unixodbc-dev for compiling pyodbc # python3-dev, default-libmysqlclient-dev, build-essential for compiling MySQL RUN --mount=type=cache,target=/var/cache/apt \\ --mount=type=cache,target=/var/lib/apt \\ apt-get update && apt-get install -y \\ g++ \\ python3-dev \\ build-essential poetry : COPY pyproject.toml poetry.lock /app/ COPY wheels/ /app/wheels/ RUN --mount=type=cache,target=/root/.cache \\ poetry install --no-dev --no-interaction See moby/buildkit docker \u00b6","title":"Cache package manager installs with buildkit"},{"location":"_til/cache-package-manager-installs-with-buildkit/#cache-package-manager-installs-with-buildkit","text":"Downloading project dependencies for every Docker rebuild is slow. buildkit enables caching downloads locally between builds thus skipping this download. Lifesaver. apt : # g++, unixodbc-dev for compiling pyodbc # python3-dev, default-libmysqlclient-dev, build-essential for compiling MySQL RUN --mount=type=cache,target=/var/cache/apt \\ --mount=type=cache,target=/var/lib/apt \\ apt-get update && apt-get install -y \\ g++ \\ python3-dev \\ build-essential poetry : COPY pyproject.toml poetry.lock /app/ COPY wheels/ /app/wheels/ RUN --mount=type=cache,target=/root/.cache \\ poetry install --no-dev --no-interaction See moby/buildkit","title":"Cache package manager installs with buildkit"},{"location":"_til/cache-package-manager-installs-with-buildkit/#docker","text":"","title":"docker"},{"location":"_til/cache-wget-installs-with-buildkit/","text":"How to cache wget installs with buildkit \u00b6 I want to use Buildkit cache to save files downloaded via wget . I can skip a downloading a file using -nc / --no-clobber to skip overwriting a file that already exists but I cannot use this alongside -O / --output-document if I want to specify the filename. I also cannot use the more intelligent -N / --timestamping which also checks if the local version is newer than the remote version for the same reason. I can, however, manually check a file exists with test ... test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz Now to use this alongside Buildkit cache. Mimicking package managers, I can save the file to /root/.wheels and copy it across ... RUN --mount=type=cache,target=/root/.wheels \\ mkdir /app/wheels && \\ test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz && \\ cp /root/.wheels/basemap-1.2.2rel.tar.gz /app/wheels/basemap-1.2.2rel.tar.gz docker \u00b6","title":"How to cache wget installs with buildkit"},{"location":"_til/cache-wget-installs-with-buildkit/#how-to-cache-wget-installs-with-buildkit","text":"I want to use Buildkit cache to save files downloaded via wget . I can skip a downloading a file using -nc / --no-clobber to skip overwriting a file that already exists but I cannot use this alongside -O / --output-document if I want to specify the filename. I also cannot use the more intelligent -N / --timestamping which also checks if the local version is newer than the remote version for the same reason. I can, however, manually check a file exists with test ... test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz Now to use this alongside Buildkit cache. Mimicking package managers, I can save the file to /root/.wheels and copy it across ... RUN --mount=type=cache,target=/root/.wheels \\ mkdir /app/wheels && \\ test -f /root/.wheels/basemap-1.2.2rel.tar.gz || \\ wget -O /root/.wheels/basemap-1.2.2rel.tar.gz https://github.com/matplotlib/basemap/archive/refs/tags/v1.2.2rel.tar.gz && \\ cp /root/.wheels/basemap-1.2.2rel.tar.gz /app/wheels/basemap-1.2.2rel.tar.gz","title":"How to cache wget installs with buildkit"},{"location":"_til/cache-wget-installs-with-buildkit/#docker","text":"","title":"docker"},{"location":"_til/connect-to-mssql-db-via-sqlalchemy/","text":"Connect to mssql db \u00b6 I want to upload csv data to a Microsoft SQL Server ( mssql ) database. I can use pandas to read & wrangle the csv data and sqlalchemy and pyodbc to upload it. To authenticate my flow I can pass in my credentials using a config dict loaded from a .env file (or prefect secrets) ... import sqlalchemy as sa connection_string = ( \"mssql+pyodbc://\" f\"{config['MSSQL_USER']}:{config['MSSQL_PASSWORD']}\" f\"@{config['MSSQL_HOST']}:{config['MSSQL_PORT']}\" f\"/{db_name}\" f\"?driver={config['MSSQL_DRIVER']}\" ) engine = sa.create_engine(connection_string) table_name = \"my_table\" df.to_sql(name=table_name, con=engine, if_exists=\"append\") ... where MSSQL_USER has the necessary permissions to create a new table if necessary. I can set these permissions using SQL Server Management Studio (SSMS) by: Adding a new user to Security > Logins with: User Mapping - add db_datareader , db_datawriter , db_ddladmin Securables - add server SERVER_NAME Vefifying user permissions on database via Databases > DATABASE_NAME > Properties > Permissions > Effective sql \u00b6","title":"Connect to mssql db"},{"location":"_til/connect-to-mssql-db-via-sqlalchemy/#connect-to-mssql-db","text":"I want to upload csv data to a Microsoft SQL Server ( mssql ) database. I can use pandas to read & wrangle the csv data and sqlalchemy and pyodbc to upload it. To authenticate my flow I can pass in my credentials using a config dict loaded from a .env file (or prefect secrets) ... import sqlalchemy as sa connection_string = ( \"mssql+pyodbc://\" f\"{config['MSSQL_USER']}:{config['MSSQL_PASSWORD']}\" f\"@{config['MSSQL_HOST']}:{config['MSSQL_PORT']}\" f\"/{db_name}\" f\"?driver={config['MSSQL_DRIVER']}\" ) engine = sa.create_engine(connection_string) table_name = \"my_table\" df.to_sql(name=table_name, con=engine, if_exists=\"append\") ... where MSSQL_USER has the necessary permissions to create a new table if necessary. I can set these permissions using SQL Server Management Studio (SSMS) by: Adding a new user to Security > Logins with: User Mapping - add db_datareader , db_datawriter , db_ddladmin Securables - add server SERVER_NAME Vefifying user permissions on database via Databases > DATABASE_NAME > Properties > Permissions > Effective","title":"Connect to mssql db"},{"location":"_til/connect-to-mssql-db-via-sqlalchemy/#sql","text":"","title":"sql"},{"location":"_til/create-a-test-mysql-database-via-pytest-django/","text":"Create a test mysql database via pytest-django \u00b6 I want to use pytest-django to spin up a test MySQL database for every test run. By default pytest-django creates a test database called test_NAME , and the default USER does not have the necessary PRIVILEGES to create it. Yurii Halapup from this Stackoverflow thread recommends explictely naming the test database in settings.py ... DATABASES = { 'default': { ... 'TEST': { 'NAME': 'test_finance', }, } } ... starting the MySQL shell ... mysql -u root -p ... and running ... GRANT ALL PRIVILEGES ON test_NAME.* TO 'USER'@'HOST'; The test database creation now works as expected django \u00b6","title":"Create a test mysql database via pytest-django"},{"location":"_til/create-a-test-mysql-database-via-pytest-django/#create-a-test-mysql-database-via-pytest-django","text":"I want to use pytest-django to spin up a test MySQL database for every test run. By default pytest-django creates a test database called test_NAME , and the default USER does not have the necessary PRIVILEGES to create it. Yurii Halapup from this Stackoverflow thread recommends explictely naming the test database in settings.py ... DATABASES = { 'default': { ... 'TEST': { 'NAME': 'test_finance', }, } } ... starting the MySQL shell ... mysql -u root -p ... and running ... GRANT ALL PRIVILEGES ON test_NAME.* TO 'USER'@'HOST'; The test database creation now works as expected","title":"Create a test mysql database via pytest-django"},{"location":"_til/create-a-test-mysql-database-via-pytest-django/#django","text":"","title":"django"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/","text":"Estimate wind speed forecast uncertainty \u00b6 Measurement accuracy \u00b6 On-site wind monitoring \u00b6 Type Description Causes Estimate? Reduce? Instrument accuracy - Calibration and mounting arrangement of the instruments ~2-3% ~0.2-0.3% by calibration Measurement interference - Anemometer mounting, type of tower, echoes from nearby objects - - Measurement inconsistency - Icing or equipment malfunction - - Second-order effects - Over-speeding, degradation or air density variations - - Long-term wind resource extrapolation \u00b6 Also known as long-term measurement higher wind regime Type Description Causes Estimate? Reduce? On-site data synthesis - Strength of the correlations between mast locations, amount of data synthesised - - Consistency of reference data - Level, nature & metadata of regional validation available - - Correlation to reference station - - - - Representativeness of period of data - How well the record period represents the long-term wind conditions inter-annual variability / SQRT(years of measurement data) - Historical wind frequency distribution - Uncertainty of the wind speed distribution measured over the period of data collected 2% / SQRT(years of measurement data) - Other \u00b6 Type Description Causes Estimate? Reduce? Vertical wind resource extrapolation Hub height shear values estimated up from mast values Representativeness of masts and heights, consistency of shear between towers, atmospheric stability and measurement configurations - - Input data accuracy - Validity of the topographic map, land cover map, position and height of obstacles to the flow as windbreaks for both the historical and future period of operation - - Spatial wind resource extrapolation Turbine values estimated across from mast values Representativeness of measurement locations vs turbine locations, complexity of the wind flow at the site (variations in ground cover) model vs measured wind speeds - Loss factor uncertainty \u00b6 Also known as energy production analysis Type Description Causes Estimate? Reduce? Wakes - Representativeness of model normal distribution with a standard deviation of ~25-35% of the wake effect - Availability Availability of the wind turbine, substation & electrical grid \"\" weibull distribution with a standard deviation of 3% - Electrical - \"\" normal distribution with a standard deviation of 10% of the loss - Turbine performance - \"\" normal distribution with a standard deviation of 2-3% of the loss - Environmental - \"\" normal distribution with a standard deviation of 10% of the loss - Curtailment - \"\" normal distribution with a standard deviation of 10% of the loss - Inter-annual variability \u00b6 Type Description Causes Estimate? Reduce? Future wind frequency distribution Year-to-year variability of wind speed distribution and air density - ~2% - Inter-annual variability Year-to-year variation of average wind speed - inter-annual variation / SQRT(10), ~6% Take a longer time-period wind-energy \u00b6","title":"Estimate wind speed forecast uncertainty"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#estimate-wind-speed-forecast-uncertainty","text":"","title":"Estimate wind speed forecast uncertainty"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#measurement-accuracy","text":"","title":"Measurement accuracy"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#on-site-wind-monitoring","text":"Type Description Causes Estimate? Reduce? Instrument accuracy - Calibration and mounting arrangement of the instruments ~2-3% ~0.2-0.3% by calibration Measurement interference - Anemometer mounting, type of tower, echoes from nearby objects - - Measurement inconsistency - Icing or equipment malfunction - - Second-order effects - Over-speeding, degradation or air density variations - -","title":"On-site wind monitoring"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#long-term-wind-resource-extrapolation","text":"Also known as long-term measurement higher wind regime Type Description Causes Estimate? Reduce? On-site data synthesis - Strength of the correlations between mast locations, amount of data synthesised - - Consistency of reference data - Level, nature & metadata of regional validation available - - Correlation to reference station - - - - Representativeness of period of data - How well the record period represents the long-term wind conditions inter-annual variability / SQRT(years of measurement data) - Historical wind frequency distribution - Uncertainty of the wind speed distribution measured over the period of data collected 2% / SQRT(years of measurement data) -","title":"Long-term wind resource extrapolation"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#other","text":"Type Description Causes Estimate? Reduce? Vertical wind resource extrapolation Hub height shear values estimated up from mast values Representativeness of masts and heights, consistency of shear between towers, atmospheric stability and measurement configurations - - Input data accuracy - Validity of the topographic map, land cover map, position and height of obstacles to the flow as windbreaks for both the historical and future period of operation - - Spatial wind resource extrapolation Turbine values estimated across from mast values Representativeness of measurement locations vs turbine locations, complexity of the wind flow at the site (variations in ground cover) model vs measured wind speeds -","title":"Other"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#loss-factor-uncertainty","text":"Also known as energy production analysis Type Description Causes Estimate? Reduce? Wakes - Representativeness of model normal distribution with a standard deviation of ~25-35% of the wake effect - Availability Availability of the wind turbine, substation & electrical grid \"\" weibull distribution with a standard deviation of 3% - Electrical - \"\" normal distribution with a standard deviation of 10% of the loss - Turbine performance - \"\" normal distribution with a standard deviation of 2-3% of the loss - Environmental - \"\" normal distribution with a standard deviation of 10% of the loss - Curtailment - \"\" normal distribution with a standard deviation of 10% of the loss -","title":"Loss factor uncertainty"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#inter-annual-variability","text":"Type Description Causes Estimate? Reduce? Future wind frequency distribution Year-to-year variability of wind speed distribution and air density - ~2% - Inter-annual variability Year-to-year variation of average wind speed - inter-annual variation / SQRT(10), ~6% Take a longer time-period","title":"Inter-annual variability"},{"location":"_til/estimate-wind-speed-forecast-uncertainty/#wind-energy","text":"","title":"wind-energy"},{"location":"_til/flag-wind-turbine-timestamps-in-fault-time-intervals/","text":"Flag wind turbine timestamps in fault time intervals \u00b6 I have turbine timeseries readings & turbine fault flags ... time ... 2017-12-31 17:00:00 ... 2017-12-31 17:10:00 ... 2017-12-31 17:20:00 ... 2017-12-31 17:30:00 ... ... fault_start_time fault_end_time ... ... 2017-12-31 17:02:00 2017-12-31 17:12:00 ... ... and I want to know if there has been any wind turbine faults during any 10 minute time interval time ... turbine_fault_occurs 2017-12-31 17:00:00 ... False 2017-12-31 17:10:00 ... True 2017-12-31 17:20:00 ... True 2017-12-31 17:30:00 ... False How do I adapt my fault time interval into the same 10 minute interval as my timeseries data? Lets first code these expectations as a unit test on which we can test our implementation ... import pandas as pd from pandas.testing import assert_frame_equal def test_flag_turbine_fault_occurences() -> None: timeseries = pd.DataFrame( { \"time\": [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], }, dtype=\"datetime64[s]\", ) faults = pd.DataFrame( { \"fault_start_time\": [\"2017-12-31 17:02:00\"], \"fault_end_time\": [\"2017-12-31 17:12:00\"], }, dtype=\"datetime64[s]\", ) expected_output = pd.DataFrame( { \"time\": pd.Series( [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], dtype=\"datetime64[s]\", ), \"turbine_fault_occurs\": [False, True, True, False], }, ) output = flag_turbine_fault_occurences( timeseries, faults, column_name_map={ \"time\": \"time\", \"fault_start_time\": \"fault_start_time\", \"fault_end_time\": \"fault_end_time\", }, ) assert_frame_equal(output, expected_output) Now for implementation ... We need to round each timestamp to the nearest 10 minute interval so fault_start_time 2017-12-31 17:02:00 becomes 2017-12-31 17:10:00 and fault_end_time 2017-12-31 17:12:00 becomes 2017-12-31 17:20:00 to capture the time interval in which either fault occurs. import pandas as pd from pandas.testing import assert_series_equal def test_ceil_to_nearest_10Min_interval() -> None: s = pd.Series([\"2017-12-31 17:02:00\", \"2017-12-31 17:58:00\"], dtype=\"datetime64[s]\") expected_output = pd.Series( [\"2017-12-31 17:10:00\", \"2017-12-31 18:00:00\"], dtype=\"datetime64[s]\" ) output = ceil_to_nearest_10Min_interval(s) assert_series_equal(output, expected_output) def ceil_to_nearest_10Min_interval(s: pd.Series) -> pd.Series: return s + pd.to_timedelta(-s.dt.minute % 10, unit='Min') Now we can call this ceil function to band the timestamps and link each start and end fault time with its corresponding time interval with a simple LEFT merge, which ensures we don't lose any time intervals for which no fault occurs. Lastly, we can check whether or not a fault begins or ends within each time interval and flag this with True or False ```python from typing import Dict import pandas as pd def flag_turbine_fault_occurences( timeseries: pd.DataFrame, faults: pd.DataFrame, column_name_map: Dict[str, str] ) -> pd.DataFrame: faults = faults.copy() c_time = column_name_map[\"time\"] c_fault_start_time = column_name_map[\"fault_start_time\"] c_fault_end_time = column_name_map[\"fault_end_time\"] fault_start_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_start_time] ) fault_end_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_end_time] ) timeseries_with_faults = ( timeseries.merge( fault_start_time_interval, left_on=c_time, right_on=c_fault_start_time, how=\"left\" ) .merge( fault_end_time_interval, left_on=c_time, right_on=c_fault_end_time, how=\"left\" ) ) timeseries_with_faults[\"turbine_fault_occurs\"] = ( (timeseries_with_faults[c_fault_start_time].notnull()) | (timeseries_with_faults[c_fault_end_time].notnull()) ) return timeseries_with_faults.drop(columns=[c_fault_start_time, c_fault_end_time]) ``` Inspired by how-to-join-two-dataframes-for-which-column-values-are-within-a-certain-range & merge-pandas-dataframes-where-one-value-is-between-two-others pandas \u00b6","title":"Flag wind turbine timestamps in fault time intervals"},{"location":"_til/flag-wind-turbine-timestamps-in-fault-time-intervals/#flag-wind-turbine-timestamps-in-fault-time-intervals","text":"I have turbine timeseries readings & turbine fault flags ... time ... 2017-12-31 17:00:00 ... 2017-12-31 17:10:00 ... 2017-12-31 17:20:00 ... 2017-12-31 17:30:00 ... ... fault_start_time fault_end_time ... ... 2017-12-31 17:02:00 2017-12-31 17:12:00 ... ... and I want to know if there has been any wind turbine faults during any 10 minute time interval time ... turbine_fault_occurs 2017-12-31 17:00:00 ... False 2017-12-31 17:10:00 ... True 2017-12-31 17:20:00 ... True 2017-12-31 17:30:00 ... False How do I adapt my fault time interval into the same 10 minute interval as my timeseries data? Lets first code these expectations as a unit test on which we can test our implementation ... import pandas as pd from pandas.testing import assert_frame_equal def test_flag_turbine_fault_occurences() -> None: timeseries = pd.DataFrame( { \"time\": [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], }, dtype=\"datetime64[s]\", ) faults = pd.DataFrame( { \"fault_start_time\": [\"2017-12-31 17:02:00\"], \"fault_end_time\": [\"2017-12-31 17:12:00\"], }, dtype=\"datetime64[s]\", ) expected_output = pd.DataFrame( { \"time\": pd.Series( [ \"2017-12-31 17:00:00\", \"2017-12-31 17:10:00\", \"2017-12-31 17:20:00\", \"2017-12-31 17:30:00\", ], dtype=\"datetime64[s]\", ), \"turbine_fault_occurs\": [False, True, True, False], }, ) output = flag_turbine_fault_occurences( timeseries, faults, column_name_map={ \"time\": \"time\", \"fault_start_time\": \"fault_start_time\", \"fault_end_time\": \"fault_end_time\", }, ) assert_frame_equal(output, expected_output) Now for implementation ... We need to round each timestamp to the nearest 10 minute interval so fault_start_time 2017-12-31 17:02:00 becomes 2017-12-31 17:10:00 and fault_end_time 2017-12-31 17:12:00 becomes 2017-12-31 17:20:00 to capture the time interval in which either fault occurs. import pandas as pd from pandas.testing import assert_series_equal def test_ceil_to_nearest_10Min_interval() -> None: s = pd.Series([\"2017-12-31 17:02:00\", \"2017-12-31 17:58:00\"], dtype=\"datetime64[s]\") expected_output = pd.Series( [\"2017-12-31 17:10:00\", \"2017-12-31 18:00:00\"], dtype=\"datetime64[s]\" ) output = ceil_to_nearest_10Min_interval(s) assert_series_equal(output, expected_output) def ceil_to_nearest_10Min_interval(s: pd.Series) -> pd.Series: return s + pd.to_timedelta(-s.dt.minute % 10, unit='Min') Now we can call this ceil function to band the timestamps and link each start and end fault time with its corresponding time interval with a simple LEFT merge, which ensures we don't lose any time intervals for which no fault occurs. Lastly, we can check whether or not a fault begins or ends within each time interval and flag this with True or False ```python from typing import Dict import pandas as pd def flag_turbine_fault_occurences( timeseries: pd.DataFrame, faults: pd.DataFrame, column_name_map: Dict[str, str] ) -> pd.DataFrame: faults = faults.copy() c_time = column_name_map[\"time\"] c_fault_start_time = column_name_map[\"fault_start_time\"] c_fault_end_time = column_name_map[\"fault_end_time\"] fault_start_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_start_time] ) fault_end_time_interval = ceil_to_nearest_10Min_interval( faults[c_fault_end_time] ) timeseries_with_faults = ( timeseries.merge( fault_start_time_interval, left_on=c_time, right_on=c_fault_start_time, how=\"left\" ) .merge( fault_end_time_interval, left_on=c_time, right_on=c_fault_end_time, how=\"left\" ) ) timeseries_with_faults[\"turbine_fault_occurs\"] = ( (timeseries_with_faults[c_fault_start_time].notnull()) | (timeseries_with_faults[c_fault_end_time].notnull()) ) return timeseries_with_faults.drop(columns=[c_fault_start_time, c_fault_end_time]) ``` Inspired by how-to-join-two-dataframes-for-which-column-values-are-within-a-certain-range & merge-pandas-dataframes-where-one-value-is-between-two-others","title":"Flag wind turbine timestamps in fault time intervals"},{"location":"_til/flag-wind-turbine-timestamps-in-fault-time-intervals/#pandas","text":"","title":"pandas"},{"location":"_til/installing-powershell-modules-on-first-launch/","text":"Installing powershell modules on first launch \u00b6 I wanted to setup my $PROFILE so that any external modules ( oh-my-posh , posh-git and powershell-git-aliases ) install on first launch. I added a simple if statement to the top of my profile ( here ) if (-Not (Get-Module -Name oh-my-posh)) { Install-Module oh-my-posh -Scope CurrentUser } if (-Not (Get-Module -Name git-aliases)) { Install-Module git-aliases -Scope CurrentUser } This creates two problems: If one of the modules is not at the latest version, powershell tries to install it alongside the existing version in a new directory alongside the old version so I then have oh-my-posh > 5.9.0 > 5.12.1 For whatever reason it's a really really slow operation to run on every startup: 16888ms vs 1892ms For now I'm keeping all external modules as a comment at the bottom of my profile :exclamation: See profile here powershell \u00b6","title":"Installing powershell modules on first launch"},{"location":"_til/installing-powershell-modules-on-first-launch/#installing-powershell-modules-on-first-launch","text":"I wanted to setup my $PROFILE so that any external modules ( oh-my-posh , posh-git and powershell-git-aliases ) install on first launch. I added a simple if statement to the top of my profile ( here ) if (-Not (Get-Module -Name oh-my-posh)) { Install-Module oh-my-posh -Scope CurrentUser } if (-Not (Get-Module -Name git-aliases)) { Install-Module git-aliases -Scope CurrentUser } This creates two problems: If one of the modules is not at the latest version, powershell tries to install it alongside the existing version in a new directory alongside the old version so I then have oh-my-posh > 5.9.0 > 5.12.1 For whatever reason it's a really really slow operation to run on every startup: 16888ms vs 1892ms For now I'm keeping all external modules as a comment at the bottom of my profile :exclamation: See profile here","title":"Installing powershell modules on first launch"},{"location":"_til/installing-powershell-modules-on-first-launch/#powershell","text":"","title":"powershell"},{"location":"_til/login-to-a-django-app-via-selenium-and-pytest-django/","text":"Login to a django app via selenium and pytest django \u00b6 I wanted to run end to end browser tests using Selenium and pytest-django . I struggled to adapt marcgibbons/django-selenium-docker from unittest to pytest . I couldn't authenticate the automatically authenticated user or admin_user generated by a pytest-django pytest fixture. I was able to verify that pytest-django does in fact create an authenticated user by dropping a breakpoint into my functional test ... from django.contrib.auth import authenticate from django.contrib.auth.models import User @pytest.mark.django_db(transaction=True) def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: breakpoint() >> User.objects.all() >> authenticate(user=\"admin\", password=\"password\") ... so the issue wasn't pytest-django user creation. I tried to manually login to the website as @marcgibbons does ... from urllib.parse import urlparse from django.conf import Settings from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture(autouse=True) def override_allowed_hosts(settings: Settings) -> None: settings.ALLOWED_HOSTS = [\"*\"] # Disable ALLOW_HOSTS @pytest.fixture def live_server_url() -> str: # Set host to externally accessible web server address return str(LiveServer(addr=\"django\")) @pytest.fixture def browser() -> webdriver.Remote: browser_ = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) yield browser_ browser_.quit() @pytest.mark.django_db def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: \"\"\" As a superuser with valid credentials, I should gain access to the Django admin. \"\"\" browser.get(live_server_url) username_input = browser.find_element_by_name('username') username_input.send_keys('admin') password_input = browser.find_element_by_name('password') password_input.send_keys('password') browser.find_element_by_xpath('//input[@value=\"Log in\"]').click() path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this didn't work either. I tried overriding my browser's cookies with the admin_user fixture credentials ... Adapted from @aljosa implementation in an issue on the pytest-django GitHub from urllib.parse import urlparse from django.conf import settings from django.conf import Settings from django.test.client import Client from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture def authenticated_browser( admin_client: Client, browser: webdriver.Remote, live_server_url: str ) -> webdriver.Remote: browser.get(live_server_url) sessionid = admin_client.cookies[\"sessionid\"] cookie = { 'name': settings.SESSION_COOKIE_NAME, 'value': sessionid.value, 'path': '/' } browser.add_cookie(cookie) browser.refresh() return browser @pytest.mark.django_db def test_auto_admin_user_login( live_server_url: str, authenticated_browser: webdriver.Remote, admin_user: User ) -> None: browser = authenticated_browser browser.get(live_server_url) path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this failed too! It turns out that the user credentials must be stored in the django website database or else it won't be able to access them! This means that we need to allow pytest-django to migrate the credentials into the test database in order to login via selenium. I can just replace ... @pytest.mark.django_db ... with ... @pytest.mark.django_db(transaction=True) ... and I can login as expected docker \u00b6","title":"Login to a django app via selenium and pytest django"},{"location":"_til/login-to-a-django-app-via-selenium-and-pytest-django/#login-to-a-django-app-via-selenium-and-pytest-django","text":"I wanted to run end to end browser tests using Selenium and pytest-django . I struggled to adapt marcgibbons/django-selenium-docker from unittest to pytest . I couldn't authenticate the automatically authenticated user or admin_user generated by a pytest-django pytest fixture. I was able to verify that pytest-django does in fact create an authenticated user by dropping a breakpoint into my functional test ... from django.contrib.auth import authenticate from django.contrib.auth.models import User @pytest.mark.django_db(transaction=True) def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: breakpoint() >> User.objects.all() >> authenticate(user=\"admin\", password=\"password\") ... so the issue wasn't pytest-django user creation. I tried to manually login to the website as @marcgibbons does ... from urllib.parse import urlparse from django.conf import Settings from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture(autouse=True) def override_allowed_hosts(settings: Settings) -> None: settings.ALLOWED_HOSTS = [\"*\"] # Disable ALLOW_HOSTS @pytest.fixture def live_server_url() -> str: # Set host to externally accessible web server address return str(LiveServer(addr=\"django\")) @pytest.fixture def browser() -> webdriver.Remote: browser_ = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) yield browser_ browser_.quit() @pytest.mark.django_db def test_manual_admin_user_login( live_server_url: str, browser: webdriver.Remote, admin_user: User ) -> None: \"\"\" As a superuser with valid credentials, I should gain access to the Django admin. \"\"\" browser.get(live_server_url) username_input = browser.find_element_by_name('username') username_input.send_keys('admin') password_input = browser.find_element_by_name('password') password_input.send_keys('password') browser.find_element_by_xpath('//input[@value=\"Log in\"]').click() path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this didn't work either. I tried overriding my browser's cookies with the admin_user fixture credentials ... Adapted from @aljosa implementation in an issue on the pytest-django GitHub from urllib.parse import urlparse from django.conf import settings from django.conf import Settings from django.test.client import Client from django.contrib.auth.models import User import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities @pytest.fixture def authenticated_browser( admin_client: Client, browser: webdriver.Remote, live_server_url: str ) -> webdriver.Remote: browser.get(live_server_url) sessionid = admin_client.cookies[\"sessionid\"] cookie = { 'name': settings.SESSION_COOKIE_NAME, 'value': sessionid.value, 'path': '/' } browser.add_cookie(cookie) browser.refresh() return browser @pytest.mark.django_db def test_auto_admin_user_login( live_server_url: str, authenticated_browser: webdriver.Remote, admin_user: User ) -> None: browser = authenticated_browser browser.get(live_server_url) path = urlparse(browser.current_url).path assert path == '/' body_text = browser.find_element_by_tag_name('body').text assert 'WELCOME, ADMIN.' in body_text ... this failed too! It turns out that the user credentials must be stored in the django website database or else it won't be able to access them! This means that we need to allow pytest-django to migrate the credentials into the test database in order to login via selenium. I can just replace ... @pytest.mark.django_db ... with ... @pytest.mark.django_db(transaction=True) ... and I can login as expected","title":"Login to a django app via selenium and pytest django"},{"location":"_til/login-to-a-django-app-via-selenium-and-pytest-django/#docker","text":"","title":"docker"},{"location":"_til/mock-requesting-a-file-from-an-external-website-in-pytest/","text":"Mock requesting a file from an external website in pytest \u00b6 I'm developing a streamlit web application that requests a 1GB zip file from an external website, unzips it, cleans it, zips it and returns this new file to the user upon request. I want to mock out the call to this external website to encapsulate my functional & unit tests as it would be very inefficient to have to download this file on every test run. I first need to create a file BERPublicsearch.txt within a zip archive BERPublicsearch.zip ... @pytest.fixture def sample_bers(tmp_path: Path) -> BytesIO: bers = pd.read_csv(\"sample-BERPublicsearch.txt\", sep=\"\\t\") f = bers.to_csv(index=False, sep=\"\\t\") filepath = tmp_path / \"BERPublicsearch.zip\" with ZipFile(filepath, \"w\") as zf: zf.writestr(\"BERPublicsearch.txt\", f) return ZipFile(filepath).read(\"BERPublicsearch.txt\") ... where sample-BERPublicsearch.txt is a 100 row sample of the data set saved in source control and sample_bers returns a bytes representation of the zip file. I can now use the responses library to mock a request to the SEAI website and return this sample zip file ... from io import BytesIO import json from pathlib import Path from time import sleep from zipfile import ZipFile import pandas as pd import pytest import responses from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities with open(\"defaults.json\") as f: DEFAULTS = json.load(f) @responses.activate def test_user_can_download_default_bers( ..., sample_bers: BytesIO ) -> None: responses.add( responses.POST, DEFAULTS[\"download\"][\"url\"], body=sample_bers, content_type=\"application/x-zip-compressed\", headers={ \"content-disposition\": \"attachment; filename=BERPublicSearch.zip\" }, status=200, ) ... where all POST request arguments are saved in defaults.json . This could be improved by using pytest fixtures to encapsulate this logic in a function that could be called by any test to mock out this call to SEAI thus avoiding some duplication. pytest \u00b6","title":"Mock requesting a file from an external website in pytest"},{"location":"_til/mock-requesting-a-file-from-an-external-website-in-pytest/#mock-requesting-a-file-from-an-external-website-in-pytest","text":"I'm developing a streamlit web application that requests a 1GB zip file from an external website, unzips it, cleans it, zips it and returns this new file to the user upon request. I want to mock out the call to this external website to encapsulate my functional & unit tests as it would be very inefficient to have to download this file on every test run. I first need to create a file BERPublicsearch.txt within a zip archive BERPublicsearch.zip ... @pytest.fixture def sample_bers(tmp_path: Path) -> BytesIO: bers = pd.read_csv(\"sample-BERPublicsearch.txt\", sep=\"\\t\") f = bers.to_csv(index=False, sep=\"\\t\") filepath = tmp_path / \"BERPublicsearch.zip\" with ZipFile(filepath, \"w\") as zf: zf.writestr(\"BERPublicsearch.txt\", f) return ZipFile(filepath).read(\"BERPublicsearch.txt\") ... where sample-BERPublicsearch.txt is a 100 row sample of the data set saved in source control and sample_bers returns a bytes representation of the zip file. I can now use the responses library to mock a request to the SEAI website and return this sample zip file ... from io import BytesIO import json from pathlib import Path from time import sleep from zipfile import ZipFile import pandas as pd import pytest import responses from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities with open(\"defaults.json\") as f: DEFAULTS = json.load(f) @responses.activate def test_user_can_download_default_bers( ..., sample_bers: BytesIO ) -> None: responses.add( responses.POST, DEFAULTS[\"download\"][\"url\"], body=sample_bers, content_type=\"application/x-zip-compressed\", headers={ \"content-disposition\": \"attachment; filename=BERPublicSearch.zip\" }, status=200, ) ... where all POST request arguments are saved in defaults.json . This could be improved by using pytest fixtures to encapsulate this logic in a function that could be called by any test to mock out this call to SEAI thus avoiding some duplication.","title":"Mock requesting a file from an external website in pytest"},{"location":"_til/mock-requesting-a-file-from-an-external-website-in-pytest/#pytest","text":"","title":"pytest"},{"location":"_til/monkeypatch-a-database-connection-in-pytest/","text":"How to monkeypatch a database connection \u00b6 I want to test that a method Datasource.fetch_dataframe raises a VPN connection error if the user is connected to StationManager but is not connected to a VPN. I'm not interested in connecting to a database, only in how this method reacts to a database connection error pyodbc.OperationalError . I tried using mock.MagicMock to replace pyodbc with a magic object that does nothing but return pyodbc.OperationalError when pyodbc.connect is called so that the database connection error is raised every time the test is run ... from unittest.mock import MagicMock import pyodbc from _pytest.monkeypatch import MonkeyPatch from stationmanager.models import Datasource def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: # ... other setup ... monkeypatch.setattr( pyodbc, \"connect\", MagicMock(return_value=pyodbc.OperationalError) ) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) ... however this merely returns the pyodbc.OperationalError and does not raise an error. It must instead be raised explicitely ... def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: def _mock_pyodbc_connect(*args, **kwargs) -> None: raise pyodbc.OperationalError # ... other setup ... monkeypatch.setattr(pyodbc, \"connect\", _mock_pyodbc_connect) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) pytest \u00b6","title":"How to monkeypatch a database connection"},{"location":"_til/monkeypatch-a-database-connection-in-pytest/#how-to-monkeypatch-a-database-connection","text":"I want to test that a method Datasource.fetch_dataframe raises a VPN connection error if the user is connected to StationManager but is not connected to a VPN. I'm not interested in connecting to a database, only in how this method reacts to a database connection error pyodbc.OperationalError . I tried using mock.MagicMock to replace pyodbc with a magic object that does nothing but return pyodbc.OperationalError when pyodbc.connect is called so that the database connection error is raised every time the test is run ... from unittest.mock import MagicMock import pyodbc from _pytest.monkeypatch import MonkeyPatch from stationmanager.models import Datasource def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: # ... other setup ... monkeypatch.setattr( pyodbc, \"connect\", MagicMock(return_value=pyodbc.OperationalError) ) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True) ... however this merely returns the pyodbc.OperationalError and does not raise an error. It must instead be raised explicitely ... def test_fetch_dataframe_raises_vpn_error(monkeypatch: MonkeyPatch) -> None: def _mock_pyodbc_connect(*args, **kwargs) -> None: raise pyodbc.OperationalError # ... other setup ... monkeypatch.setattr(pyodbc, \"connect\", _mock_pyodbc_connect) datasource = Datasource() with pytest.raises(pyodbc.OperationalError): datasource.fetch_dataframe(raw_db_table=True)","title":"How to monkeypatch a database connection"},{"location":"_til/monkeypatch-a-database-connection-in-pytest/#pytest","text":"","title":"pytest"},{"location":"_til/restore-a-mysql-database-on-docker-compose/","text":"Restore a mysql database on docker compose \u00b6 This one was tough. I had a Docker Compose yml of ... version: \"3.7\" services: db: image: mysql volumes: - ./db:/var/lib/mysql - ./backups/datadump.sql:/docker-entrypoint-initdb.d/datadump.sql environment: - MYSQL_DATABASE=$MYSQL_NAME - MYSQL_USER=$MYSQL_USER - MYSQL_PASSWORD=$MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD ... web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/app ports: - \"8000:8000\" depends_on: - db ... and couldn't work out how to alter the database permissions so that I could access the database from a host other than the db localhost. Specifically, I kept running into user 'username'@'IP' cannot access 'databasename' when launching Docker Compose . From the MySQL Docker docs (and Stackoverflow and Docker Community ...) I found that setting the environment variable MYSQL_ROOT_HOST to % enables accessing the database from a any IP address. After much pain I found out (by reading the Shell script docker-entrypoint.sh for this container) that MYSQL_ROOT_HOST is automatically set to % ! This meant that the datadump.sql file being run to restore the database overwrites user permissions! A quick scan of the sql file validated this! Now how do I overwrite the overwrite!? The bloody MySQL docs have the answer! I just needed to ... ... run mysqld --skip-grant-tables to enable connecting to the server without a password and with all privileges ... ... - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD command: --skip-grant-tables ... ... launch the database ... docker compose up db ... hook into the running database server ... docker exec -it stationmanager-db-1 mysql ... tell the server to reload the grant tables so that account-management statements work .. FLUSH PRIVILEGES; ... add the user permissions needed to access the database ... ALTER USER 'username'@'%' IDENTIFIED BY 'MyNewPass'; GRANT ALL ON *.* TO 'username'@'%' WITH GRANT OPTION ; ... and finally the Docker web container can now connect to the db server which is running our restored database! docker \u00b6","title":"Restore a mysql database on docker compose"},{"location":"_til/restore-a-mysql-database-on-docker-compose/#restore-a-mysql-database-on-docker-compose","text":"This one was tough. I had a Docker Compose yml of ... version: \"3.7\" services: db: image: mysql volumes: - ./db:/var/lib/mysql - ./backups/datadump.sql:/docker-entrypoint-initdb.d/datadump.sql environment: - MYSQL_DATABASE=$MYSQL_NAME - MYSQL_USER=$MYSQL_USER - MYSQL_PASSWORD=$MYSQL_PASSWORD - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD ... web: build: . command: python manage.py runserver 0.0.0.0:8000 volumes: - .:/app ports: - \"8000:8000\" depends_on: - db ... and couldn't work out how to alter the database permissions so that I could access the database from a host other than the db localhost. Specifically, I kept running into user 'username'@'IP' cannot access 'databasename' when launching Docker Compose . From the MySQL Docker docs (and Stackoverflow and Docker Community ...) I found that setting the environment variable MYSQL_ROOT_HOST to % enables accessing the database from a any IP address. After much pain I found out (by reading the Shell script docker-entrypoint.sh for this container) that MYSQL_ROOT_HOST is automatically set to % ! This meant that the datadump.sql file being run to restore the database overwrites user permissions! A quick scan of the sql file validated this! Now how do I overwrite the overwrite!? The bloody MySQL docs have the answer! I just needed to ... ... run mysqld --skip-grant-tables to enable connecting to the server without a password and with all privileges ... ... - MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD command: --skip-grant-tables ... ... launch the database ... docker compose up db ... hook into the running database server ... docker exec -it stationmanager-db-1 mysql ... tell the server to reload the grant tables so that account-management statements work .. FLUSH PRIVILEGES; ... add the user permissions needed to access the database ... ALTER USER 'username'@'%' IDENTIFIED BY 'MyNewPass'; GRANT ALL ON *.* TO 'username'@'%' WITH GRANT OPTION ; ... and finally the Docker web container can now connect to the db server which is running our restored database!","title":"Restore a mysql database on docker compose"},{"location":"_til/restore-a-mysql-database-on-docker-compose/#docker","text":"","title":"docker"},{"location":"_til/run-django-via-docker-compose/","text":"How to run django via docker compose \u00b6 I want to run a Django application on docker-compose so that I can define the required infrastructure and dependencies as code and run this application on any machine. The Official Docker Django tutorial demonstrates how to setup a minimal Django application on docker-compose . I want to use a mysql database instead of postgres which means that I need to configure my docker-compose.yml differently. Following the Official MySQL Docker guide I can use ... ... version: '3.7' services: db: image: mysql volumes: - ./data/db:/var/lib/mysql environment: - MYSQL_DATABASE=mysql - MYSQL_USER=mysql - MYSQL_PASSWORD=mysql - MYSQL_RANDOM_ROOT_PASSWORD=1 ports: - \"3406:3306\" web: build: . command: python manage.py runserver 0.0.0.0:8000 ports: - \"8000:8000\" volumes: - .:/app depends_on: - db ... Note: - I had to change the default port on my host ( Windows Subsystem for Linux 2 ) to 3406 as 3306 wasn't available - how-do-i-change-the-default-port-on-which-my-docker-mysql-instance-runs - I tried adding /tmp/app/mysqld:/var/run/mysqld and /tmp/app/mysqld:/run/mysqld as volumes to db and web respectively to map the contents of mysqld to a local folder as suggested by dockerizing-a-django-mysql-project-g4m but ran into permissions issues whereby the web container couldn't write mysqlx.sock.lock to the /run/mysqld ! This file contains socket information that enables the web service to talk to the database service I also need to edit my application's settings.py to point to this docker-compose database ... DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mysql', 'USER': 'mysql', 'PASSWORD': 'mysql', 'HOST': 'db', 'PORT': 3306, } } Now when I run docker-compose up , both images build fine but MySQL causes problems ... django-container-deployment-mysql-docker-deployment/ suggests installing a Python MySQL connector via pip install mysqlclient in the Django application to enable this connection between db and web which requires system dependencies in the Dockerfile ... ... RUN apt update && apt install -y \\ g++ \\ default-libmysqlclient-dev \\ python3-dev \\ build-essential ... docker \u00b6","title":"How to run django via docker compose"},{"location":"_til/run-django-via-docker-compose/#how-to-run-django-via-docker-compose","text":"I want to run a Django application on docker-compose so that I can define the required infrastructure and dependencies as code and run this application on any machine. The Official Docker Django tutorial demonstrates how to setup a minimal Django application on docker-compose . I want to use a mysql database instead of postgres which means that I need to configure my docker-compose.yml differently. Following the Official MySQL Docker guide I can use ... ... version: '3.7' services: db: image: mysql volumes: - ./data/db:/var/lib/mysql environment: - MYSQL_DATABASE=mysql - MYSQL_USER=mysql - MYSQL_PASSWORD=mysql - MYSQL_RANDOM_ROOT_PASSWORD=1 ports: - \"3406:3306\" web: build: . command: python manage.py runserver 0.0.0.0:8000 ports: - \"8000:8000\" volumes: - .:/app depends_on: - db ... Note: - I had to change the default port on my host ( Windows Subsystem for Linux 2 ) to 3406 as 3306 wasn't available - how-do-i-change-the-default-port-on-which-my-docker-mysql-instance-runs - I tried adding /tmp/app/mysqld:/var/run/mysqld and /tmp/app/mysqld:/run/mysqld as volumes to db and web respectively to map the contents of mysqld to a local folder as suggested by dockerizing-a-django-mysql-project-g4m but ran into permissions issues whereby the web container couldn't write mysqlx.sock.lock to the /run/mysqld ! This file contains socket information that enables the web service to talk to the database service I also need to edit my application's settings.py to point to this docker-compose database ... DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mysql', 'USER': 'mysql', 'PASSWORD': 'mysql', 'HOST': 'db', 'PORT': 3306, } } Now when I run docker-compose up , both images build fine but MySQL causes problems ... django-container-deployment-mysql-docker-deployment/ suggests installing a Python MySQL connector via pip install mysqlclient in the Django application to enable this connection between db and web which requires system dependencies in the Dockerfile ... ... RUN apt update && apt install -y \\ g++ \\ default-libmysqlclient-dev \\ python3-dev \\ build-essential ...","title":"How to run django via docker compose"},{"location":"_til/run-django-via-docker-compose/#docker","text":"","title":"docker"},{"location":"_til/running-scripts-on-launch/","text":"Running scripts on launch \u00b6 I wanted to set some aliases for conda in a separate conda-aliases.ps1 script and activate custom oh-my-posh configuration rdmolony.omp.json on startup both of which are saved in the same directory as $PROFILE so I can track them in the same repository. Running . .\\conda-aliases.ps1 works fine when opening in a shell in the $PROFILE directory, however, when changing directory powershell can no longer find the file! I need to pass an absolute path to the script instead, however, I need to make use of an environmental variable holding the $PROFILE directory to make this generalisable. I can use either (Get-ChildItem $PROFILE) (see here ) or for the parent of any file $PSScriptRoot) ( here ) :book: I can use dir env: to get a list of all environmental variables Replacing . \".\\conda-aliases.ps1\" With . (Join-Path $PSScriptRoot \".\\conda-aliases.ps1\") Does the trick ( here ! :exclamation: See profile here powershell \u00b6","title":"Running scripts on launch"},{"location":"_til/running-scripts-on-launch/#running-scripts-on-launch","text":"I wanted to set some aliases for conda in a separate conda-aliases.ps1 script and activate custom oh-my-posh configuration rdmolony.omp.json on startup both of which are saved in the same directory as $PROFILE so I can track them in the same repository. Running . .\\conda-aliases.ps1 works fine when opening in a shell in the $PROFILE directory, however, when changing directory powershell can no longer find the file! I need to pass an absolute path to the script instead, however, I need to make use of an environmental variable holding the $PROFILE directory to make this generalisable. I can use either (Get-ChildItem $PROFILE) (see here ) or for the parent of any file $PSScriptRoot) ( here ) :book: I can use dir env: to get a list of all environmental variables Replacing . \".\\conda-aliases.ps1\" With . (Join-Path $PSScriptRoot \".\\conda-aliases.ps1\") Does the trick ( here ! :exclamation: See profile here","title":"Running scripts on launch"},{"location":"_til/running-scripts-on-launch/#powershell","text":"","title":"powershell"},{"location":"_til/select-all-text-between-two-strings-across-lines-via-regex/","text":"Select all text between two strings across lines via regex \u00b6 I want to remove a block of text from the docstrings of 20+ functions at once where each block of text coincides with .. plot:: and \"\"\" ... ... .. plot:: :include-source: import mrp from mrp import crosstab test_station = mrp.load_test_station(mrp.__file__) ct = crosstab.heightbydata( test_station, test_station.t1avg, 2 ) crosstab.plot(ct, show_values=False) \"\"\" I can't use the Meta Escape character . as this does not include line terminators \\n . I can, however, use [\\w\\W] , which means select all words and non-words, alongside the one or more lazy quantifier +? where ? forces it to match as few words as possible ... (.. plot::[\\w\\W]+?)\"\"\" regex \u00b6","title":"Select all text between two strings across lines via regex"},{"location":"_til/select-all-text-between-two-strings-across-lines-via-regex/#select-all-text-between-two-strings-across-lines-via-regex","text":"I want to remove a block of text from the docstrings of 20+ functions at once where each block of text coincides with .. plot:: and \"\"\" ... ... .. plot:: :include-source: import mrp from mrp import crosstab test_station = mrp.load_test_station(mrp.__file__) ct = crosstab.heightbydata( test_station, test_station.t1avg, 2 ) crosstab.plot(ct, show_values=False) \"\"\" I can't use the Meta Escape character . as this does not include line terminators \\n . I can, however, use [\\w\\W] , which means select all words and non-words, alongside the one or more lazy quantifier +? where ? forces it to match as few words as possible ... (.. plot::[\\w\\W]+?)\"\"\"","title":"Select all text between two strings across lines via regex"},{"location":"_til/select-all-text-between-two-strings-across-lines-via-regex/#regex","text":"","title":"regex"},{"location":"_til/styling-powershell-with-oh-my-posh/","text":"Styling Powershell with oh-my-posh \u00b6 Coming from oh-my-zsh I wanted to style my prompt to a fancy theme and enable answering the following questions with my shell prompt: What git branch am I on? Am I up to date with upstream? Is my virtual environment activated? The only catch was installing a font which can display all of the emojis used by each theme. nerd-fonts Meslo is recommended by oh-my-posh , however, even the mono Meslo fonts have too much whitespace between characters and look weird in vscode . The powerlevel10k MesloLGS NF Regular.ttf does work. After installing my font I checked it was installed by going to C:\\Windows\\Fonts and finally I copied and pasted the filename into Settings > Terminal > Font Family to set it as the default in vscode :exclamation: See profile here powershell \u00b6","title":"Styling Powershell with [`oh-my-posh`](https://github.com/jandedobbeleer/oh-my-posh)"},{"location":"_til/styling-powershell-with-oh-my-posh/#styling-powershell-with-oh-my-posh","text":"Coming from oh-my-zsh I wanted to style my prompt to a fancy theme and enable answering the following questions with my shell prompt: What git branch am I on? Am I up to date with upstream? Is my virtual environment activated? The only catch was installing a font which can display all of the emojis used by each theme. nerd-fonts Meslo is recommended by oh-my-posh , however, even the mono Meslo fonts have too much whitespace between characters and look weird in vscode . The powerlevel10k MesloLGS NF Regular.ttf does work. After installing my font I checked it was installed by going to C:\\Windows\\Fonts and finally I copied and pasted the filename into Settings > Terminal > Font Family to set it as the default in vscode :exclamation: See profile here","title":"Styling Powershell with oh-my-posh"},{"location":"_til/styling-powershell-with-oh-my-posh/#powershell","text":"","title":"powershell"},{"location":"_til/switch-selenium-user-within-a-test/","text":"Switch selenium user within a test \u00b6 I want to set up and tear down a Selenium webdriver twice in the same test so that I can test how my Django app behaves for multiple users. By default Selenium hangs if the test function test_multiple_users_can_start_lists_at_different_urls fails at any point beyond quitting and restarting the browser ... import re import socket import time import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def _get_remote_webdriver() -> webdriver.Remote: return webdriver.Remote( command_executor=\"http://selenium:4444/wd/hub\", desired_capabilities=DesiredCapabilities.CHROME, ) @pytest.fixture def webdriver_init() -> webdriver.Remote: browser = _get_remote_webdriver() yield browser browser.quit() def _get_web_container_ipaddess() -> str: host_name = socket.gethostname() host_ipaddress = socket.gethostbyname(host_name) return host_ipaddress @pytest.fixture def live_server_at_web_container_ipaddress() -> LiveServer: # Set host to externally accessible web server address web_container_ip_address = _get_web_container_ipaddess() return LiveServer(addr=web_container_ip_address) @pytest.mark.django_db def test_multiple_users_can_start_lists_at_different_urls( webdriver_init: webdriver.Remote, live_server_at_web_container_ipaddress: LiveServer, ) -> None: browser = webdriver_init live_server_url = str(live_server_at_web_container_ipaddress) # Edith starts a new to-do list browser.get(live_server_url) inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy peacock feathers\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy peacock feathers\") # She notices that her list has a unique URL edith_list_url = browser.current_url assert re.search( \"/lists/.+\", edith_list_url ), f\"Regex didn't match: 'lists/.+' not found in {edith_list_url}\" # Now a new user, Francis, comes along to the site. ## We use a new browser session to make sure that no information ## of Edith's is coming through from cookies etc browser.quit() browser = _get_remote_webdriver() # Francis visits the home page. There is no sign of Edith's # list browser.get(live_server_url) page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"make a fly\" not in page_text # Francis starts a list by entering a new item. He # is less interesting than Edith... inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy milk\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy milk\") # Francis gets his own unique URL francis_list_url = browser.current_url assert re.search( \"/lists/.+\", francis_list_url ), f\"Regex didn't match: 'lists/.+' not found in {francis_list_url}\" assert francis_list_url != edith_list_url # Again, there is no trace of Edith's list page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"Buy milk\" in page_text # Satisfied, they both go back to sleep I added a breakpoint after yield in webdriver_init ... > /app/functional_tests/tests.py(34)webdriver_init() -> browser.quit() (Pdb) browser.quit() *** selenium.common.exceptions.WebDriverException: Message: Unable to execute request for an existing session: Unable to find session with ID: 0b215f3be5545e376c41ba5d1695e6d6 Build info: version: '4.1.0', revision: '87802e897b' System info: host: 'f9d7fe69645a', ip: '172.18.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '5.10.60.1-microsoft-standard-WSL2', java.version: '11.0.11' Driver info: driver.version: unknown ... which seems to fail because the 2nd browser session is created with a different session_id to the 1st browser session and so Selenium cannot quit a browser session which is already over. I'm hacking around this by replacing ... ... browser.quit() browser = _get_remote_webdriver() ... ... with ... ... browser.delete_all_cookies() ... pytest \u00b6","title":"Switch selenium user within a test"},{"location":"_til/switch-selenium-user-within-a-test/#switch-selenium-user-within-a-test","text":"I want to set up and tear down a Selenium webdriver twice in the same test so that I can test how my Django app behaves for multiple users. By default Selenium hangs if the test function test_multiple_users_can_start_lists_at_different_urls fails at any point beyond quitting and restarting the browser ... import re import socket import time import pytest from pytest_django.live_server_helper import LiveServer from selenium import webdriver from selenium.webdriver.common.keys import Keys from selenium.webdriver.common.desired_capabilities import DesiredCapabilities def _get_remote_webdriver() -> webdriver.Remote: return webdriver.Remote( command_executor=\"http://selenium:4444/wd/hub\", desired_capabilities=DesiredCapabilities.CHROME, ) @pytest.fixture def webdriver_init() -> webdriver.Remote: browser = _get_remote_webdriver() yield browser browser.quit() def _get_web_container_ipaddess() -> str: host_name = socket.gethostname() host_ipaddress = socket.gethostbyname(host_name) return host_ipaddress @pytest.fixture def live_server_at_web_container_ipaddress() -> LiveServer: # Set host to externally accessible web server address web_container_ip_address = _get_web_container_ipaddess() return LiveServer(addr=web_container_ip_address) @pytest.mark.django_db def test_multiple_users_can_start_lists_at_different_urls( webdriver_init: webdriver.Remote, live_server_at_web_container_ipaddress: LiveServer, ) -> None: browser = webdriver_init live_server_url = str(live_server_at_web_container_ipaddress) # Edith starts a new to-do list browser.get(live_server_url) inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy peacock feathers\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy peacock feathers\") # She notices that her list has a unique URL edith_list_url = browser.current_url assert re.search( \"/lists/.+\", edith_list_url ), f\"Regex didn't match: 'lists/.+' not found in {edith_list_url}\" # Now a new user, Francis, comes along to the site. ## We use a new browser session to make sure that no information ## of Edith's is coming through from cookies etc browser.quit() browser = _get_remote_webdriver() # Francis visits the home page. There is no sign of Edith's # list browser.get(live_server_url) page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"make a fly\" not in page_text # Francis starts a list by entering a new item. He # is less interesting than Edith... inputbox = browser.find_element_by_id(\"id_new_item\") inputbox.send_keys(\"Buy milk\") inputbox.send_keys(Keys.ENTER) wait_for_row_in_list_table(browser, \"1: Buy milk\") # Francis gets his own unique URL francis_list_url = browser.current_url assert re.search( \"/lists/.+\", francis_list_url ), f\"Regex didn't match: 'lists/.+' not found in {francis_list_url}\" assert francis_list_url != edith_list_url # Again, there is no trace of Edith's list page_text = browser.find_element_by_tag_name(\"body\").text assert \"Buy peacock feathers\" not in page_text assert \"Buy milk\" in page_text # Satisfied, they both go back to sleep I added a breakpoint after yield in webdriver_init ... > /app/functional_tests/tests.py(34)webdriver_init() -> browser.quit() (Pdb) browser.quit() *** selenium.common.exceptions.WebDriverException: Message: Unable to execute request for an existing session: Unable to find session with ID: 0b215f3be5545e376c41ba5d1695e6d6 Build info: version: '4.1.0', revision: '87802e897b' System info: host: 'f9d7fe69645a', ip: '172.18.0.2', os.name: 'Linux', os.arch: 'amd64', os.version: '5.10.60.1-microsoft-standard-WSL2', java.version: '11.0.11' Driver info: driver.version: unknown ... which seems to fail because the 2nd browser session is created with a different session_id to the 1st browser session and so Selenium cannot quit a browser session which is already over. I'm hacking around this by replacing ... ... browser.quit() browser = _get_remote_webdriver() ... ... with ... ... browser.delete_all_cookies() ...","title":"Switch selenium user within a test"},{"location":"_til/switch-selenium-user-within-a-test/#pytest","text":"","title":"pytest"},{"location":"_til/test-a-django-app-on-docker-via-selenium-2/","text":"Test a django app on docker via selenium 2 \u00b6 Marc Gibbons blog provides a nice working example but it doesn't explain how it works in as much detail as I'd like. Marc uses Docker links to enable referring to the selenium container using its name rather than its IP address ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 - 5900:5900 ... which enables hooking into the Selenium RemoteDriver running elsewhere ... from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities browser = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) For the following code ... @tag('selenium') @override_settings(ALLOWED_HOSTS=['*']) class BaseTestCase(StaticLiveServerTestCase): \"\"\" Provides base test class which connects to the Docker container running selenium. \"\"\" host = '0.0.0.0' @classmethod def setUpClass(cls): super().setUpClass() cls.host = socket.gethostbyname(socket.gethostname()) cls.selenium = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) cls.selenium.implicitly_wait(5) ... host defines running a StaticLiveServerTestCase similar to python manage.py runserver 0.0.0.0:8000 , and setting cls.host to socket.gethostbyname(socket.gethostname()) only impacts self.live_server_url by replacing 0.0.0.0 with the docker container IP address. This enables accessing the running web container from the selenium container. I adapt this example to run on pytest instead of unittest via pytest-django at rdmolony/django-selenium-pytest-docker I struggled to adapt Harry Percival's obeythetestinggoat examples initial functional test example. In this case I want to run the Selenium RemoteDriver on a running web server (started via python manage.py runserver 0.0.0.0:8000 ). To access this running server from the Selenium browser I created http://{host}/8000 where host is socket.gethostbyname(socket.gethostname()) . host was 172.22.0.4 instead of 172.22.0.3 . It turns out that I was creating multiple shells in the web container incorrectly. I launched web via ... docker compose run --name goat-web --rm web python manage.py runserver 0.0.0.0:8000 ... and I hooked into this running server to run my functional tests via ... docker compose run --rm web python functional_tests.py ... which is wrong! Hooking into web creates a separate container with a separate IP address. If instead I hook into the running server via exec instead of run it works fine ... docker exec -it goat-web python functional_tests.py I got sidetracked reading into Docker networking. It turns out that Docker Compose creates a network on docker compose up which means that web can access selenium via http://selenium instead of running its IP address and so does not need to be explicitely linked via links ! docker \u00b6","title":"Test a django app on docker via selenium 2"},{"location":"_til/test-a-django-app-on-docker-via-selenium-2/#test-a-django-app-on-docker-via-selenium-2","text":"Marc Gibbons blog provides a nice working example but it doesn't explain how it works in as much detail as I'd like. Marc uses Docker links to enable referring to the selenium container using its name rather than its IP address ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 - 5900:5900 ... which enables hooking into the Selenium RemoteDriver running elsewhere ... from selenium import webdriver from selenium.webdriver.common.desired_capabilities import DesiredCapabilities browser = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) For the following code ... @tag('selenium') @override_settings(ALLOWED_HOSTS=['*']) class BaseTestCase(StaticLiveServerTestCase): \"\"\" Provides base test class which connects to the Docker container running selenium. \"\"\" host = '0.0.0.0' @classmethod def setUpClass(cls): super().setUpClass() cls.host = socket.gethostbyname(socket.gethostname()) cls.selenium = webdriver.Remote( command_executor='http://selenium:4444/wd/hub', desired_capabilities=DesiredCapabilities.CHROME, ) cls.selenium.implicitly_wait(5) ... host defines running a StaticLiveServerTestCase similar to python manage.py runserver 0.0.0.0:8000 , and setting cls.host to socket.gethostbyname(socket.gethostname()) only impacts self.live_server_url by replacing 0.0.0.0 with the docker container IP address. This enables accessing the running web container from the selenium container. I adapt this example to run on pytest instead of unittest via pytest-django at rdmolony/django-selenium-pytest-docker I struggled to adapt Harry Percival's obeythetestinggoat examples initial functional test example. In this case I want to run the Selenium RemoteDriver on a running web server (started via python manage.py runserver 0.0.0.0:8000 ). To access this running server from the Selenium browser I created http://{host}/8000 where host is socket.gethostbyname(socket.gethostname()) . host was 172.22.0.4 instead of 172.22.0.3 . It turns out that I was creating multiple shells in the web container incorrectly. I launched web via ... docker compose run --name goat-web --rm web python manage.py runserver 0.0.0.0:8000 ... and I hooked into this running server to run my functional tests via ... docker compose run --rm web python functional_tests.py ... which is wrong! Hooking into web creates a separate container with a separate IP address. If instead I hook into the running server via exec instead of run it works fine ... docker exec -it goat-web python functional_tests.py I got sidetracked reading into Docker networking. It turns out that Docker Compose creates a network on docker compose up which means that web can access selenium via http://selenium instead of running its IP address and so does not need to be explicitely linked via links !","title":"Test a django app on docker via selenium 2"},{"location":"_til/test-a-django-app-on-docker-via-selenium-2/#docker","text":"","title":"docker"},{"location":"_til/test-a-django-app-on-docker-via-selenium/","text":"Test a django app on docker via selenium \u00b6 Marc Gibbons guide Selenium Tests in Django & Docker and accompanying code works like a charm on Windows Subsystems for Linux 2 (WSL2) except for the VNC Viewer which can't be installed on my work laptop without authorisation by IS. VNC means virtual network computing or remotely controlling a computer from anoter device The key is Docker Compose links ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 # Selenium - 5900:5900 # VNC Server ... which links the django container to the selenium container which means that containers for the linked service are accessible at a hostname identical to the alias. NoVNC runs on WSL2 without requiring admin priviledges. It enables hooking into the Selenium VNC server. First, I got Selenium running ... docker compose start selenium ... which starts a VNC Server at localhost:5900 which I can hook into with NoVNC by running the novnc_proxy Shell script ... git clone https://github.com/novnc/noVNC cd noVNC ./utils/novnc_proxy --vnc localhost:5900 ... and access in my browser via localhost:5900/vnc.html . I can now run my Django app unit tests and view their execution via NoVNC via ... docker compose run django python manage.py test docker \u00b6","title":"Test a django app on docker via selenium"},{"location":"_til/test-a-django-app-on-docker-via-selenium/#test-a-django-app-on-docker-via-selenium","text":"Marc Gibbons guide Selenium Tests in Django & Docker and accompanying code works like a charm on Windows Subsystems for Linux 2 (WSL2) except for the VNC Viewer which can't be installed on my work laptop without authorisation by IS. VNC means virtual network computing or remotely controlling a computer from anoter device The key is Docker Compose links ... version: '3' services: django: build: . volumes: - \".:/code\" links: - selenium ports: - \"8000:8000\" selenium: image: selenium/standalone-chrome-debug:3.7.1 ports: - 4444:4444 # Selenium - 5900:5900 # VNC Server ... which links the django container to the selenium container which means that containers for the linked service are accessible at a hostname identical to the alias. NoVNC runs on WSL2 without requiring admin priviledges. It enables hooking into the Selenium VNC server. First, I got Selenium running ... docker compose start selenium ... which starts a VNC Server at localhost:5900 which I can hook into with NoVNC by running the novnc_proxy Shell script ... git clone https://github.com/novnc/noVNC cd noVNC ./utils/novnc_proxy --vnc localhost:5900 ... and access in my browser via localhost:5900/vnc.html . I can now run my Django app unit tests and view their execution via NoVNC via ... docker compose run django python manage.py test","title":"Test a django app on docker via selenium"},{"location":"_til/test-a-django-app-on-docker-via-selenium/#docker","text":"","title":"docker"},{"location":"_til/test-huey-tasks/","text":"Test huey tasks \u00b6 I want to unit test some huey tasks that are periodically run by Django to import csv files to a MSSQL database. To do so I must run them in immediate mode , however, this is set in settings.py which is tricky to override in pytest-django . pytest-django implements a pytest fixture called settings that enables overriding settings.py on a test-by-test basis. So I tried settings.HUEY['immediate'] = True prior to importing the huey task. Checking the huey task via import_datafile.huey.immediate shows this doesn't work. Instead, I can override the task manually via import_datafile.huey.immediate = True prior to calling it. This does the trick django \u00b6","title":"Test huey tasks"},{"location":"_til/test-huey-tasks/#test-huey-tasks","text":"I want to unit test some huey tasks that are periodically run by Django to import csv files to a MSSQL database. To do so I must run them in immediate mode , however, this is set in settings.py which is tricky to override in pytest-django . pytest-django implements a pytest fixture called settings that enables overriding settings.py on a test-by-test basis. So I tried settings.HUEY['immediate'] = True prior to importing the huey task. Checking the huey task via import_datafile.huey.immediate shows this doesn't work. Instead, I can override the task manually via import_datafile.huey.immediate = True prior to calling it. This does the trick","title":"Test huey tasks"},{"location":"_til/test-huey-tasks/#django","text":"","title":"django"}]}